{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_prep = pd.read_csv(\"../data/weather_data_prep.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert DATE column into Date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_prep['DATE']= pd.to_datetime(weather_data_prep['DATE']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop useless columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think SNOW, SNWD (Snow Depth) and PGTM (Peak gust time) are useless because too many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_prep = weather_data_prep.drop(['SNWD', 'PGTM', 'SNOW'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_prep = weather_data_prep.groupby(['DATE']).agg({\n",
    "    'AWND': 'mean',\n",
    "    'PRCP':'mean',\n",
    "    'TAVG': 'mean',\n",
    "    'WDF5': 'mean',\n",
    "    'TMAX':'max',\n",
    "    'WSF2': 'max',\n",
    "    'WSF5': 'max',\n",
    "    'WT01': 'max',\n",
    "    'WT02': 'max',\n",
    "    'WT03': 'max',\n",
    "    'WT08': 'max',\n",
    "    'TMIN': 'min'\n",
    "})\n",
    "\n",
    "\n",
    "weather_data_prep = weather_data_prep.rename(columns={\n",
    "    \"AWND\": \"AWND_mean\", \n",
    "    \"PRCP\": \"PRCP_mean\", \n",
    "    \"TAVG\": \"TAVG_mean\", \n",
    "    \"WDF5\": \"WDF5_mean\", \n",
    "    \"TMAX\": \"TMAX_max\",\n",
    "    'WSF2': 'WSF2_max',\n",
    "    'WSF5': 'WSF5_max',\n",
    "    'WT01': 'WT01_max',\n",
    "    'WT02': 'WT02_max',\n",
    "    'WT03': 'WT03_max',\n",
    "    'WT08': 'WT08_max',\n",
    "    'TMIN': 'TMIN_min'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add two informations that might affect the prediction: if there is ice on the road, if the road is wet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_data_prep['IceRoad'] = np.where(weather_data_prep['TMIN_min'] < 37.4, 1, 0) #Conversion: 37.4F = 3Â°C\n",
    "weather_data_prep['WetDay'] = np.where(weather_data_prep['PRCP_mean'] > 0, 1, 0)\n",
    "weather_data_prep = weather_data_prep.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airport dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyprien\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Airport_Data = pd.read_csv(\"../data/Airport_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop useless columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed the instructions given in the glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_Data = Airport_Data.drop([\n",
    "    'stand_last_change', \n",
    "    'sto',\n",
    "    'atot',\n",
    "    'aobt',\n",
    "    'chocks_on',\n",
    "    'stand_scheduled',\n",
    "    'last_distance_to_gate',\n",
    "    'last_in_sector',\n",
    "    'status',\n",
    "    'mode_s',\n",
    "    'acReg',\n",
    "    'partition',\n",
    "    'vdgs_in',\n",
    "    'stand_active',\n",
    "    'stand_docking',\n",
    "    'aibt_received',\n",
    "    'sqt',\n",
    "    'plb_on',\n",
    "    'pca_on',\n",
    "    'gpu_on',\n",
    "    'towbar_on',\n",
    "    'plb_off',\n",
    "    'pca_off',\n",
    "    'gpu_off',\n",
    "    'acars_out',\n",
    "    'vdgs_out',\n",
    "    'stand_free',\n",
    "    'eobt',\n",
    "    'aldt_received',\n",
    "    'stand_prepared',\n",
    "    'stand_auto_start',\n",
    "    'roll',\n",
    "    'speed'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Date column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_Data['DATE'] = pd.to_datetime(Airport_Data['aldt'], errors='coerce').dt.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather = pd.merge(Airport_Data, weather_data_prep, how='left', on=['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather = Airport_and_weather.rename(columns={\n",
    "    \"carrier\": \"Airline\", \n",
    "    \"flight\": \"FlightNumber\", \n",
    "    \"DATE\": \"Date\", \n",
    "    \"acType\": \"AircraftType\", \n",
    "    \"ship\": \"ShipmentWeight\",\n",
    "    'runway': 'Runway',\n",
    "    'stand': 'Stand',\n",
    "    'aldt': 'ActualLandingTime',\n",
    "    'eibt': 'EstimatedInBlockTime',\n",
    "    'cibt': 'CalculatedInBlockTime',\n",
    "    'aibt': 'ActualInBlockTime'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Taxi time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather['TaxiTime'] = pd.to_datetime(Airport_and_weather['ActualInBlockTime'], errors='coerce') - pd.to_datetime(Airport_and_weather['ActualLandingTime'], errors='coerce')\n",
    "\n",
    "\n",
    "Airport_and_weather['TaxiTime'] = Airport_and_weather['TaxiTime'].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop duplicates and the rows containing NAN values for Taxi time, and the rows containing absurd taxi times values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather = Airport_and_weather.drop_duplicates()\n",
    "Airport_and_weather = Airport_and_weather[np.isfinite(Airport_and_weather['TaxiTime'])]\n",
    "Airport_and_weather = Airport_and_weather[(Airport_and_weather['TaxiTime'] >= 0) & (Airport_and_weather['TaxiTime'] < 120)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We separate into groups the planes according to their shipment weight, which might affect the taxi time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (pd.to_numeric(Airport_and_weather['ShipmentWeight'], errors='coerce') < 2000),\n",
    "    (pd.to_numeric(Airport_and_weather['ShipmentWeight'], errors='coerce') >= 2000) & (pd.to_numeric(Airport_and_weather['ShipmentWeight'], errors='coerce') < 6000),\n",
    "    (pd.to_numeric(Airport_and_weather['ShipmentWeight'], errors='coerce') >= 6000)]\n",
    "\n",
    "choices = ['S', 'M', 'L']\n",
    "\n",
    "Airport_and_weather['ShipmentWeightCat'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the number of planes which arrive 10mn before the one in the row. If there is a lot of planes that are landing at the same time, it will affect the taxi time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather['ActualLandingTime'] = pd.to_datetime(Airport_and_weather['ActualLandingTime'], errors='coerce')\n",
    "Airport_and_weather = Airport_and_weather.sort_values(by='ActualLandingTime').set_index('ActualLandingTime')\n",
    "Airport_and_weather['NbPlanesLast10Mn'] = Airport_and_weather['TaxiTime'].rolling(\"10T\").count()\n",
    "Airport_and_weather = Airport_and_weather.reset_index()\n",
    "\n",
    "Airport_and_weather['NbPlanesLast10Mn'] = Airport_and_weather['NbPlanesLast10Mn'] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column with the day of the week (from 0 on Monday to 6 on Sunday) and the month of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather['Hour'] = Airport_and_weather['ActualLandingTime'].dt.hour\n",
    "Airport_and_weather['DayOfTheWeek'] = Airport_and_weather['ActualLandingTime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform Runway & Stand into a numeric values for the model as well as the stand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather['Runway'] = pd.to_numeric(Airport_and_weather['Runway'].str[7:9], errors='coerce')\n",
    "Airport_and_weather = Airport_and_weather.rename(columns={\n",
    "    \"Runway\": \"RunwayNumber\"\n",
    "})\n",
    "\n",
    "Airport_and_weather['Stand'] =  pd.to_numeric(Airport_and_weather['Stand'].str[5:], errors='coerce')\n",
    "Airport_and_weather = Airport_and_weather.rename(columns={\n",
    "    \"Stand\": \"StandNumber\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the columns that are no longer needed.\n",
    "We drop TMAX_max and TMIN_min because highly correlated with TAVG_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather = Airport_and_weather.drop([\n",
    "    'FlightNumber', \n",
    "    'EstimatedInBlockTime',\n",
    "    'CalculatedInBlockTime',\n",
    "    'ActualInBlockTime',\n",
    "    'TMAX_max',\n",
    "    'TMIN_min',\n",
    "    'ShipmentWeight'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the log of the taxi time in order to have a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Airport_and_weather['LogTaxiTime'] = np.log(Airport_and_weather['TaxiTime'] + 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aircraft dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the name of the aircrafts on the two files provided (ACchar.xlsx & Airport_Data.csv) are not the same, we created a file giving the correspondence between those two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Aircrafts_info = pd.read_csv(\"../data/aircraft_simplified_data.csv\", sep=\";\",header=0,encoding = 'unicode_escape')\n",
    "Aircrafts_info = Aircrafts_info.rename(columns={\n",
    "    \"Model_Airport_Data\": \"AircraftType\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataset = pd.merge(Airport_and_weather, Aircrafts_info, how='left', on=['AircraftType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter dates that had obviously a problem and which are not relevant for the predictions (the average of the taxi time these days are more than 20 minutes, which is way higher than the other dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataset = FinalDataset[FinalDataset['Date'] != \"2018-09-26\"]\n",
    "FinalDataset = FinalDataset[FinalDataset['Date'] != \"2018-08-01\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns, drop the columns that are correlated with parking area (wingspan and length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns renaming: \n",
    "FinalDataset.rename(columns={'Parking Area':'ParkingArea',\n",
    "                             'WT01_max':'WT01max',\n",
    "                             'WT02_max':'WT02max', \n",
    "                             'WT03_max':'WT03max', \n",
    "                             'WT08_max':'WT08max', \n",
    "                             'WSF2_max':'WSF2max', \n",
    "                             'WSF5_max':'WSF5max',\n",
    "                             'Model_Acchar':'ModelAcchar',  \n",
    "                             'TAVG_mean':'TAVGmean',\n",
    "                             'AWND_mean':'AWNDmean', \n",
    "                             'TAVG_mean':'TAVGmean', \n",
    "                             'DayOfTheWeek':'WeekDay',\n",
    "                             'WDF5_mean':'WDF5mean', \n",
    "                             'PRCP_mean':'PRCPmean'}, inplace=True)\n",
    "\n",
    "# Dropping correlated variables:  \n",
    "FinalDataset=FinalDataset.drop(labels=[\"Wingspan\",\"Length\"], axis=1)\n",
    "\n",
    "# Converting dates to_datetime: \n",
    "FinalDataset.ActualLandingTime=pd.to_datetime(FinalDataset.ActualLandingTime.values)\n",
    "FinalDataset.Date=pd.to_datetime(FinalDataset.Date.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop nan values except for the weather where a null just means that it is equal to 0 (example for the rain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping (real) NA values ~4%: \n",
    "FinalDataset=FinalDataset.dropna(subset=[\"RunwayNumber\", \"StandNumber\", \"AircraftType\",\"ParkingArea\"])\n",
    "\n",
    "# Filling other missing values with 0 : \n",
    "FinalDataset=FinalDataset.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stand Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StandAnalysis = FinalDataset.loc[:,[\"StandNumber\",\"TaxiTime\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SortedStand = StandAnalysis.groupby(\"StandNumber\").mean().reset_index().sort_values(\"TaxiTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FirstGroup = SortedStand.iloc[0:40,:].loc[:,\"StandNumber\"].tolist()\n",
    "SecondGroup = SortedStand.iloc[40:80,:].loc[:,\"StandNumber\"].tolist()\n",
    "ThirdGroup = SortedStand.iloc[80:120,:].loc[:,\"StandNumber\"].tolist()\n",
    "ForthGroup = SortedStand.iloc[120:,:].loc[:,\"StandNumber\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Stands = FinalDataset[\"StandNumber\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StandRank = []\n",
    "for i in range(len(Stands)):\n",
    "    if Stands[i] in FirstGroup :\n",
    "        StandRank.append(0)\n",
    "    elif Stands[i] in SecondGroup :\n",
    "        StandRank.append(1)\n",
    "    elif Stands[i] in ThirdGroup :\n",
    "        StandRank.append(2)\n",
    "    else :\n",
    "        StandRank.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataset[\"StandRank\"] = StandRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FinalDataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Putting TaxiTime and LogTaxiTime as last columns : \n",
    "taxitime=FinalDataset.TaxiTime\n",
    "logtaxitime=FinalDataset.LogTaxiTime \n",
    "\n",
    "FinalDataset.drop([\"TaxiTime\", \"LogTaxiTime\", \"ModelAcchar\"], axis=1, inplace=True)\n",
    "\n",
    "FinalDataset[\"TaxiTime\"]=taxitime\n",
    "FinalDataset[\"LogTaxiTime\"]=logtaxitime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalDataset[FinalDataset.isnull() == True].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_csv = FinalDataset.to_csv('../data/DatasetCleanFinal.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
